{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP Lesson 2:  additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import the modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.196989Z",
     "iopub.status.busy": "2025-05-19T21:45:38.196581Z",
     "iopub.status.idle": "2025-05-19T21:45:38.267842Z",
     "shell.execute_reply": "2025-05-19T21:45:38.267324Z"
    }
   },
   "outputs": [],
   "source": [
    "import marmote.core as mc \n",
    "import marmote.markovchain as mmc\n",
    "import marmote.mdp as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.271359Z",
     "iopub.status.busy": "2025-05-19T21:45:38.270589Z",
     "iopub.status.idle": "2025-05-19T21:45:38.274275Z",
     "shell.execute_reply": "2025-05-19T21:45:38.273873Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a model with three states *s1, s2, s3* and three actions *a0, a1, a2*. Transitions probabilities and rewards are described by the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./geron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating States**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.278593Z",
     "iopub.status.busy": "2025-05-19T21:45:38.278034Z",
     "iopub.status.idle": "2025-05-19T21:45:38.280924Z",
     "shell.execute_reply": "2025-05-19T21:45:38.280618Z"
    }
   },
   "outputs": [],
   "source": [
    "actionSpace = mc.MarmoteInterval(0,2) \n",
    "stateSpace = mc.MarmoteInterval(0,2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some modelling choices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it could be noticed, the number of actions is not the same in each state. In state *s1* one could trigger any of the actions *a0*, *a1*, and *a2* while in state *s3* only action *a2* can be triggered.  \n",
    "To make programming easier, we have chosen to have an identical action space in each state. This means that we can activate all actions *a0*, *a1*, *a2* in all state. To do this, we add missing actions which will have no effect and which will receive a high cost.  \n",
    "Hence in state *s3* we add action *a0* with a transition to *s3* with probability *1*.  \n",
    "Hence in state *s3* we add action *a0* with a transition to *s3* with probability *1*.  \n",
    "Hence in state *s2* we add action *a2* with a transition to *s2* with probability *1*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we enter the transition matrices (we do not entry the null values) and add each matricx to a list. The matrices obey to the modeling choice presented above and then have the same dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Transitions Matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.282906Z",
     "iopub.status.busy": "2025-05-19T21:45:38.282411Z",
     "iopub.status.idle": "2025-05-19T21:45:38.286893Z",
     "shell.execute_reply": "2025-05-19T21:45:38.286591Z"
    }
   },
   "outputs": [],
   "source": [
    "trans=list()\n",
    "        \n",
    "# matrix for the a_0 action    \n",
    "P0 = mc.SparseMatrix(3)\n",
    "P0.setEntry(0,0,0.7) \n",
    "P0.setEntry(0,1,0.3) \n",
    "P0.setEntry(1,1,1.0) \n",
    "P0.setEntry(2,2,1.0) \n",
    "trans.append(P0) \n",
    "\n",
    "# matrix for the a_1 action\n",
    "P1 = mc.SparseMatrix(3) \n",
    "P1.setEntry(0,0,1.0)  \n",
    "P1.setEntry(1,2,1.0) \n",
    "P1.setEntry(2,2,1.0) \n",
    "trans.append(P1) \n",
    "\n",
    "# matrix for the a_2 action    \n",
    "P2 = mc.SparseMatrix(3) \n",
    "P2.setEntry(0,0,0.8) \n",
    "P2.setEntry(0,1,0.2)\n",
    "P2.setEntry(1,1,1.0) \n",
    "P2.setEntry(2,0,0.8) \n",
    "P2.setEntry(2,1,0.1) \n",
    "P2.setEntry(2,2,0.1) \n",
    "trans.append(P2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation of several rewards matrices**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reward values depend on the transition with a cost per transition, then we  use a second way to build the MDP. \n",
    "We use a second constructor which uses two lists of matrices. One for the transition matrices as before and another for the rewards per transition. For the later list, the matrix at the *k*-th entry defines the gains associated with the action with index *k*. In this matrix, the entry with coordinate (i,j) defines the reward of the transition from i to j.\n",
    "\n",
    "We also define the penalty given to unavailable actions. Here a small negative value (-10^5) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.288839Z",
     "iopub.status.busy": "2025-05-19T21:45:38.288314Z",
     "iopub.status.idle": "2025-05-19T21:45:38.292490Z",
     "shell.execute_reply": "2025-05-19T21:45:38.291938Z"
    }
   },
   "outputs": [],
   "source": [
    "penalty = -100000 \n",
    "\n",
    "R1 =  mc.SparseMatrix(3)\n",
    "R2 =  mc.SparseMatrix(3)\n",
    "R3 =  mc.SparseMatrix(3)\n",
    "\n",
    "# fill in non null entries in sparse matrix    \n",
    "R1.setEntry(0,0,10)\n",
    "R1.setEntry(2,2,penalty)\n",
    "\n",
    "R2.setEntry(1,2,-50)\n",
    "R2.setEntry(2,2,penalty)\n",
    "    \n",
    "R3.setEntry(1,1,penalty)\n",
    "R3.setEntry(2,0,40)\n",
    "\n",
    "#Adding reward to list\n",
    "rews=list()\n",
    "rews.append(R1)\n",
    "rews.append(R2)\n",
    "rews.append(R3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.295377Z",
     "iopub.status.busy": "2025-05-19T21:45:38.294724Z",
     "iopub.status.idle": "2025-05-19T21:45:38.299669Z",
     "shell.execute_reply": "2025-05-19T21:45:38.299203Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking\n",
      "R1 [[1.000000e+01, 0.000000e+00, 0.000000e+00],\n",
      " [0.000000e+00, 0.000000e+00, 0.000000e+00],\n",
      " [0.000000e+00, 0.000000e+00, -1.000000e+05]]\n",
      "\n",
      "R2 [[0.000000e+00, 0.000000e+00, 0.000000e+00],\n",
      " [0.000000e+00, 0.000000e+00, -5.000000e+01],\n",
      " [0.000000e+00, 0.000000e+00, -1.000000e+05]]\n",
      "\n",
      "R3 [[0.000000e+00, 0.000000e+00, 0.000000e+00],\n",
      " [0.000000e+00, -1.000000e+05, 0.000000e+00],\n",
      " [4.000000e+01, 0.000000e+00, 0.000000e+00]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking\")\n",
    "print(\"R1\",str(R1))\n",
    "print(\"R2\",str(R2))\n",
    "print(\"R3\",str(R3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.302531Z",
     "iopub.status.busy": "2025-05-19T21:45:38.301878Z",
     "iopub.status.idle": "2025-05-19T21:45:38.305102Z",
     "shell.execute_reply": "2025-05-19T21:45:38.304583Z"
    }
   },
   "outputs": [],
   "source": [
    "beta = 0.95\n",
    "criterion=\"max\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.307991Z",
     "iopub.status.busy": "2025-05-19T21:45:38.307276Z",
     "iopub.status.idle": "2025-05-19T21:45:38.311746Z",
     "shell.execute_reply": "2025-05-19T21:45:38.311112Z"
    }
   },
   "outputs": [],
   "source": [
    "second_mdp = md.DiscountedMDP(criterion, stateSpace, actionSpace, trans, rews, beta) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving a value to the parameters for the solving algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.317464Z",
     "iopub.status.busy": "2025-05-19T21:45:38.316720Z",
     "iopub.status.idle": "2025-05-19T21:45:38.320033Z",
     "shell.execute_reply": "2025-05-19T21:45:38.319530Z"
    }
   },
   "outputs": [],
   "source": [
    "#create and initialize epsilon.\n",
    "epsilon = 0.00001\n",
    "#maximum number of iterations allowed.\n",
    "maxIter = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.322377Z",
     "iopub.status.busy": "2025-05-19T21:45:38.322233Z",
     "iopub.status.idle": "2025-05-19T21:45:38.324416Z",
     "shell.execute_reply": "2025-05-19T21:45:38.324161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "Solution of MDP problem\n",
      "Size of the state space: 3\n",
      "#############################################\n",
      "Solution model: Feedback Stationary Policy\n",
      "- column 1: index of the state\n",
      "- column 2: Value function \n",
      "- column 3: Optimal action \n",
      "\n",
      "  0           21.8983   0\n",
      "  1           1.17884   1\n",
      "  2           53.8725   2\n",
      "#############################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimum2 = second_mdp.ValueIteration(epsilon, maxIter)\n",
    "print(optimum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional features (Markov Chain and Q-Value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the Markov chain associated with a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition matrix associated with a policy can be retrieved using the `getChain` method, which returns a `SparseMatrix` constructed with respect to the policy given as a parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.326092Z",
     "iopub.status.busy": "2025-05-19T21:45:38.325878Z",
     "iopub.status.idle": "2025-05-19T21:45:38.328220Z",
     "shell.execute_reply": "2025-05-19T21:45:38.327965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.000000e-01, 3.000000e-01, 0.000000e+00],\n",
      " [0.000000e+00, 0.000000e+00, 1.000000e+00],\n",
      " [8.000000e-01, 1.000000e-01, 1.000000e-01]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mat=second_mdp.GetChain(optimum2)\n",
    "Mat.set_type(mc.DISCRETE)\n",
    "print(Mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No we create the Markov Chain as detailled in Lesson 1. We first create the initial distribution, then the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.329437Z",
     "iopub.status.busy": "2025-05-19T21:45:38.329317Z",
     "iopub.status.idle": "2025-05-19T21:45:38.331888Z",
     "shell.execute_reply": "2025-05-19T21:45:38.331632Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_prob = np.array( [0.333, 0.333, 0.334] )\n",
    "states=np.array([0,1,2])\n",
    "initial = mc.DiscreteDistribution(states, initial_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.333297Z",
     "iopub.status.busy": "2025-05-19T21:45:38.333110Z",
     "iopub.status.idle": "2025-05-19T21:45:38.335702Z",
     "shell.execute_reply": "2025-05-19T21:45:38.335021Z"
    }
   },
   "outputs": [],
   "source": [
    "chain = mmc.MarkovChain( Mat )\n",
    "chain.set_init_distribution(initial)\n",
    "chain.set_model_name( \"Chain issued from the MDP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.336924Z",
     "iopub.status.busy": "2025-05-19T21:45:38.336803Z",
     "iopub.status.idle": "2025-05-19T21:45:38.339750Z",
     "shell.execute_reply": "2025-05-19T21:45:38.339100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discrete sparse\n",
      "3\n",
      "         0          0 7.000000e-01\n",
      "         0          1 3.000000e-01\n",
      "         1          2 1.000000e+00\n",
      "         2          0 8.000000e-01\n",
      "         2          1 1.000000e-01\n",
      "         2          2 1.000000e-01\n",
      "stop\n",
      "discrete values { 0 1 2 } probas {    0.333    0.333    0.334 } \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the Q Value associated with a policy (for R.L. purpose) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also possible to create a `FeedbackQvalueMDP` in a `DiscountedMDP`. A `FeedbackQvalueMDP` is an object that is created form the value of a policy (in our case a `FeedbackSolutionMDP`). It stores a *Q-value*  for any couple *(s,a)* with *s* the state and *a* the action. From that, it is then possible to randomly draw actions according to the *EpsilonGreedy* or *Softmax* rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `FeedbackQvalueMDP` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.341593Z",
     "iopub.status.busy": "2025-05-19T21:45:38.341444Z",
     "iopub.status.idle": "2025-05-19T21:45:38.344395Z",
     "shell.execute_reply": "2025-05-19T21:45:38.343769Z"
    }
   },
   "outputs": [],
   "source": [
    "F=second_mdp.GetQValue(optimum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.346059Z",
     "iopub.status.busy": "2025-05-19T21:45:38.345931Z",
     "iopub.status.idle": "2025-05-19T21:45:38.348868Z",
     "shell.execute_reply": "2025-05-19T21:45:38.348404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "# - column 1: index of the state\n",
      "# - column 2: index of the action\n",
      "# - column 3: Value\n",
      "#\n",
      "0 0 21.89831521\n",
      "0 1 20.80335271\n",
      "0 2 16.86666105\n",
      "1 0 1.119894386\n",
      "1 1 1.178885397\n",
      "1 2 -99998.88011\n",
      "2 0 -99948.82111\n",
      "2 1 -99948.82111\n",
      "2 2 53.87256015\n",
      "#############################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For drawing action we should reset the random generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.350281Z",
     "iopub.status.busy": "2025-05-19T21:45:38.350157Z",
     "iopub.status.idle": "2025-05-19T21:45:38.352324Z",
     "shell.execute_reply": "2025-05-19T21:45:38.351882Z"
    }
   },
   "outputs": [],
   "source": [
    "F.ResetSeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw an action with *EpsilonGreedy* principle in state 0 with epsilon=0.1 for a maximisation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.354009Z",
     "iopub.status.busy": "2025-05-19T21:45:38.353608Z",
     "iopub.status.idle": "2025-05-19T21:45:38.356314Z",
     "shell.execute_reply": "2025-05-19T21:45:38.355867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "action=F.EpsilonGreedyMax(0,0.1)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw an action with *SoftMax* principle in state 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-19T21:45:38.357698Z",
     "iopub.status.busy": "2025-05-19T21:45:38.357478Z",
     "iopub.status.idle": "2025-05-19T21:45:38.360563Z",
     "shell.execute_reply": "2025-05-19T21:45:38.360053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "action=F.SoftMax(2)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marmote-use",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
