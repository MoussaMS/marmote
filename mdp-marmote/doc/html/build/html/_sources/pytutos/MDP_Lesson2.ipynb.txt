{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP Lesson 2: additional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import marmote.core as mc\n",
    "import marmote.markovchain as mmc\n",
    "import marmote.mdp as md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "also import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Model](#model)\n",
    "2. [Build the MDP - Constructor 1](#build-the-mdp---constructor-1)\n",
    "3. [Build the MDP - Constructor 2](#build-the-mdp---constructor-2)\n",
    "4. [Build the MDP - Constructor 3](#build-the-mdp---constructor-3)\n",
    "5. [Q-Value](#q-value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "### Description of the Model\n",
    "We use a model with three states *s1, s2, s3* and three actions *a0, a1, a2*. Transition probabilities and rewards are described by the picture below.\n",
    "\n",
    "<img src=\"./geron.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MDP - Constructor 1\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "### Creating States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionSpace = mc.MarmoteInterval(0, 2)\n",
    "stateSpace = mc.MarmoteInterval(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some modelling choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it could be noticed, the number of actions is not the same in each state. In state *s1* one could trigger any of the actions *a0*, *a1*, and *a2* while in state *s3* only action *a2* can be triggered. \n",
    "\n",
    "To make programming easier, we have chosen to have an identical action space in each state. This means that we can activate all actions *a0*, *a1*, *a2* in all state. To do this, we add missing actions which will have no effect and which will receive a high cost. \n",
    "\n",
    "Hence in state *s3* we add action *a0* with a transition to *s3* with probability *1*. \n",
    "\n",
    "Hence in state *s3* we add action *a0* with a transition to *s3* with probability *1*. \n",
    "\n",
    "Hence in state *s2* we add action *a2* with a transition to *s2* with probability *1*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we enter the transition matrices (we do not entry the null values) and add each matrix to a list. The matrices obey to the modeling choice presented above and then have the same dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MDP - Constructor 2\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "### Creating Transitions Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = list()\n",
    "\n",
    "# matrix for the a_0 action\n",
    "P0 = mc.SparseMatrix(3)\n",
    "P0.setEntry(0, 0, 0.7)\n",
    "P0.setEntry(0, 1, 0.3)\n",
    "P0.setEntry(1, 1, 1.0)\n",
    "P0.setEntry(2, 2, 1.0)\n",
    "trans.append(P0)\n",
    "\n",
    "# matrix for the a_1 action\n",
    "P1 = mc.SparseMatrix(3)\n",
    "P1.setEntry(0, 0, 1.0)\n",
    "P1.setEntry(1, 2, 1.0)\n",
    "P1.setEntry(2, 2, 1.0)\n",
    "trans.append(P1)\n",
    "\n",
    "# matrix for the a_2 action\n",
    "P2 = mc.SparseMatrix(3)\n",
    "P2.setEntry(0, 0, 0.8)\n",
    "P2.setEntry(0, 1, 0.2)\n",
    "P2.setEntry(1, 1, 1.0)\n",
    "P2.setEntry(2, 0, 0.8)\n",
    "P2.setEntry(2, 1, 0.1)\n",
    "P2.setEntry(2, 2, 0.1)\n",
    "trans.append(P2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MDP - Constructor 3\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "### Creation of several rewards matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reward values depend on the transition with a cost per transition, then we use a second way to build the MDP. \n",
    "We use a second constructor which uses two lists of matrices. One for the transition matrices as before and another for the rewards per transition. For the later list, the matrix at the *k*-th entry defines the gains associated with the action with index *k*. In this matrix, the entry with coordinate (i,j) defines the reward of the transition from i to j.\n",
    "\n",
    "We also define the penalty given to unavailable actions. Here a small negative value (-10^5) is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = -100000\n",
    "\n",
    "R1 = mc.SparseMatrix(3)\n",
    "R2 = mc.SparseMatrix(3)\n",
    "R3 = mc.SparseMatrix(3)\n",
    "\n",
    "# fill in non null entries in sparse matrix\n",
    "R1.setEntry(0, 0, 10)\n",
    "R1.setEntry(2, 2, penalty)\n",
    "\n",
    "R2.setEntry(1, 2, -50)\n",
    "R2.setEntry(2, 2, penalty)\n",
    "\n",
    "R3.setEntry(1, 1, penalty)\n",
    "R3.setEntry(2, 0, 40)\n",
    "\n",
    "# Adding reward to list\n",
    "rews = list()\n",
    "rews.append(R1)\n",
    "rews.append(R2)\n",
    "rews.append(R3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Checking\")\n",
    "print(\"R1\", str(R1))\n",
    "print(\"R2\", str(R2))\n",
    "print(\"R3\", str(R3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.95\n",
    "criterion = \"max\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_mdp = md.DiscountedMDP(criterion, stateSpace, actionSpace, trans, rews, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Value\n",
    "\n",
    "[Back to Table of Contents](#table-of-contents)\n",
    "\n",
    "### Creation of the Q Value associated with a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also possible to create a `FeedbackQvalueMDP` in a `DiscountedMDP`. A `FeedbackQvalueMDP` is an object that is created form the value of a policy (in our case a `FeedbackSolutionMDP`). It stores a *Q-value* for any couple *(s,a)* with *s* the state and *a* the action. From that, it is then possible to randomly draw actions according to the *EpsilonGreedy* or *Softmax* rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the `FeedbackQvalueMDP` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = second_mdp.GetQValue(optimum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we print it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For drawing action we should reset the random generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.ResetSeed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw an action with *EpsilonGreedy* principle in state 0 with epsilon=0.1 for a maximisation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = F.EpsilonGreedyMax(0, 0.1)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We draw an action with *SoftMax* principle in state 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = F.SoftMax(2)\n",
    "print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marmote-use",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
