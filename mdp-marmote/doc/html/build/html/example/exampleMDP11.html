

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>MDP Example 11: Additional Methods &mdash; Marmote initial documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=51ad5368"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Example MDP 21 : Average MDP and Policy handling" href="exampleMDP21.html" />
    <link rel="prev" title="MDP Example 10: Discounted MDP" href="exampleMDP10.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Marmote
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Table of Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../instructions.html">Installation instructions</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../cpp_api.html">C++ API</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../cpp_examples.html"> Commented examples</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="example1.html">Example 1</a></li>
<li class="toctree-l3"><a class="reference internal" href="example2.html">Example 2</a></li>
<li class="toctree-l3"><a class="reference internal" href="example3.html">Example 3</a></li>
<li class="toctree-l3"><a class="reference internal" href="example4.html">Example 4</a></li>
<li class="toctree-l3"><a class="reference internal" href="example5.html">Example 5</a></li>
<li class="toctree-l3"><a class="reference internal" href="example6.html">Example 6</a></li>
<li class="toctree-l3"><a class="reference internal" href="example7.html">Example 7</a></li>
<li class="toctree-l3"><a class="reference internal" href="example10.html">Example 10</a></li>
<li class="toctree-l3"><a class="reference internal" href="exampleMDP10.html">MDP Example 10: Discounted MDP</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">MDP Example 11: Additional Methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#description">Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tasks-performed">Tasks performed</a></li>
<li class="toctree-l4"><a class="reference internal" href="#code">Code</a></li>
<li class="toctree-l4"><a class="reference internal" href="#download">Download</a></li>
<li class="toctree-l4"><a class="reference internal" href="#output">Output</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="exampleMDP21.html">Example MDP 21 : Average MDP and Policy handling</a></li>
<li class="toctree-l3"><a class="reference internal" href="exampleMDP31.html">Example MDP 31 : Total Reward MDP with two-dimensional state space</a></li>
<li class="toctree-l3"><a class="reference internal" href="exampleMDP40.html">Example MDP 40</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../cpp_downloads.html"> Files for download</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cpp_api.html#topics">Topics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../python_api.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about.html">About Marmote and MarmoteMDP</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Marmote</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../cpp_api.html">C++ API</a></li>
          <li class="breadcrumb-item"><a href="../cpp_examples.html">C++ Tutorial and Examples</a></li>
      <li class="breadcrumb-item active">MDP Example 11: Additional Methods</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/example/exampleMDP11.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="mdp-example-11-additional-methods">
<h1>MDP Example 11: Additional Methods<a class="headerlink" href="#mdp-example-11-additional-methods" title="Link to this heading"></a></h1>
<section id="description">
<h2>Description<a class="headerlink" href="#description" title="Link to this heading"></a></h2>
<p>We want to implement the model of MDP proposed by A. Geron in the book
<em>Hands-On Machine Learning With Scikit-Learn and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems</em> (2017).</p>
<p>The MDP is given (Chapter 16 fig 16.8) by:
|</p>
<a class="reference internal image-reference" href="../_images/geron.png"><img alt="picture of the MDP" class="align-center" src="../_images/geron.png" style="width: 666.0px; height: 299.7px;" />
</a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>As we can see, the number of actions is not the same in each state. In state s1, any of the actions a0, a1, and a2 can be triggered, whereas in state s3, only action a2 can be triggered.</p>
<p>To simplify programming, we chose to have an identical action space in each state. This means that we can activate all actions a0, a1, a2 in all states. To do this, we add missing actions that will have no effect and will receive a high cost.</p>
<p>In state s3, we add action a0 with a transition to s3 with a probability of 1.
In state s3, we add action a1 with a transition to s3 with a probability of 1.
In state s2, we add action a2 with a transition to s2 with a probability of 1.
With a discount factor of 0.95, the expected optimal policy and value function are those mentioned in the original RST file.</p>
</section>
<section id="tasks-performed">
<h2>Tasks performed<a class="headerlink" href="#tasks-performed" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p>Creation of the MDP</p>
<ul class="simple">
<li><p>Creation of two <code class="docutils literal notranslate"><span class="pre">MarmoteInterval</span></code> objects to contain the state space and the action space</p></li>
<li><p>Creation of three <code class="docutils literal notranslate"><span class="pre">SparseMatrix</span></code> transition matrices associated with each of the three actions</p></li>
<li><p>Creation of three <code class="docutils literal notranslate"><span class="pre">SparseMatrix</span></code> reward matrices for each action</p></li>
<li><p>Creation of the discounted MDP <code class="docutils literal notranslate"><span class="pre">DiscountedMDP</span></code></p></li>
</ul>
</li>
<li><p>Solving the MDP with the ValueIteration method</p></li>
<li><p>Creation of a Markov chain associated with the optimal policy</p></li>
<li><p>Generation of Q-values for reinforcement learning</p></li>
<li><p>Use of EpsilonGreedy and SoftMax strategies for action selection</p></li>
</ol>
</section>
<section id="code">
<h2>Code<a class="headerlink" href="#code" title="Link to this heading"></a></h2>
<p>Here is the creation of the state and action spaces:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">MarmoteSet</span><span class="w"> </span><span class="o">*</span><span class="n">actionSpace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MarmoteInterval</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>
<span class="n">MarmoteSet</span><span class="w"> </span><span class="o">*</span><span class="n">stateSpace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MarmoteInterval</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>
</pre></div>
</div>
<p>Then, the creation of the transition matrices:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="n">TransitionStructure</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">trans</span><span class="p">(</span><span class="n">actionSpace</span><span class="o">-&gt;</span><span class="n">Cardinal</span><span class="p">());</span>
<span class="n">SparseMatrix</span><span class="w"> </span><span class="o">*</span><span class="n">P0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SparseMatrix</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="cm">/* matrix for the a0 action*/</span>
<span class="n">P0</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.7</span><span class="p">);</span>
<span class="n">P0</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.3</span><span class="p">);</span>
<span class="n">P0</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span>
<span class="n">P0</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span><span class="w"> </span><span class="cm">/* add virtual action */</span>
<span class="n">trans</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">P0</span><span class="p">;</span>

<span class="cm">/* matrix for the a1 action*/</span>
<span class="n">SparseMatrix</span><span class="w"> </span><span class="o">*</span><span class="n">P1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SparseMatrix</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="n">P1</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span>
<span class="n">P1</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span>
<span class="n">P1</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span><span class="w"> </span><span class="cm">/* add virtual action */</span>
<span class="n">trans</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">P1</span><span class="p">;</span>

<span class="cm">/* matrix for the a2 action*/</span>
<span class="n">SparseMatrix</span><span class="w"> </span><span class="o">*</span><span class="n">P2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SparseMatrix</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="n">P2</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.8</span><span class="p">);</span>
<span class="n">P2</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.2</span><span class="p">);</span>
<span class="n">P2</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">1.0</span><span class="p">);</span><span class="w"> </span><span class="cm">/* add virtual action */</span>
<span class="n">P2</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.8</span><span class="p">);</span>
<span class="n">P2</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">);</span>
<span class="n">P2</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">);</span>
<span class="n">trans</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">P2</span><span class="p">;</span>
</pre></div>
</div>
<p>Creation of reward matrices for each action:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">penalty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">-100000</span><span class="p">;</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">TransitionStructure</span><span class="o">*&gt;</span><span class="w"> </span><span class="n">rews</span><span class="p">(</span><span class="n">actionSpace</span><span class="o">-&gt;</span><span class="n">Cardinal</span><span class="p">());</span>

<span class="n">SparseMatrix</span><span class="w"> </span><span class="o">*</span><span class="n">R1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SparseMatrix</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="n">SparseMatrix</span><span class="w"> </span><span class="o">*</span><span class="n">R2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SparseMatrix</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
<span class="n">SparseMatrix</span><span class="w"> </span><span class="o">*</span><span class="n">R3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">SparseMatrix</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>

<span class="n">R1</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">);</span>
<span class="n">R1</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">penalty</span><span class="p">);</span>

<span class="n">R2</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">-50</span><span class="p">);</span>
<span class="n">R2</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">penalty</span><span class="p">);</span>

<span class="n">R3</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">penalty</span><span class="p">);</span>
<span class="n">R3</span><span class="o">-&gt;</span><span class="n">setEntry</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">40</span><span class="p">);</span>

<span class="cm">/* Adding reward to collection */</span>
<span class="n">rews</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">R1</span><span class="p">;</span>
<span class="n">rews</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">R2</span><span class="p">;</span>
<span class="n">rews</span><span class="p">.</span><span class="n">at</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">R3</span><span class="p">;</span>
</pre></div>
</div>
<p>Definition of parameters and creation of the MDP:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">beta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.95</span><span class="p">;</span>
<span class="n">string</span><span class="w"> </span><span class="n">critere</span><span class="p">(</span><span class="s">&quot;max&quot;</span><span class="p">);</span>

<span class="n">DiscountedMDP</span><span class="w"> </span><span class="o">*</span><span class="n">mdp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">DiscountedMDP</span><span class="p">(</span><span class="n">critere</span><span class="p">,</span><span class="w"> </span><span class="n">stateSpace</span><span class="p">,</span><span class="w"> </span><span class="n">actionSpace</span><span class="p">,</span><span class="w"> </span><span class="n">trans</span><span class="p">,</span><span class="w"> </span><span class="n">rews</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">);</span>
</pre></div>
</div>
<p>Solving the MDP with Value Iteration:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">epsilon</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.00001</span><span class="p">;</span>
<span class="kt">int</span><span class="w"> </span><span class="n">maxIter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">150</span><span class="p">;</span>

<span class="n">FeedbackSolutionMDP</span><span class="w"> </span><span class="o">*</span><span class="n">optimum</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mdp</span><span class="o">-&gt;</span><span class="n">ValueIteration</span><span class="p">(</span><span class="n">epsilon</span><span class="p">,</span><span class="w"> </span><span class="n">maxIter</span><span class="p">);</span>
<span class="n">optimum</span><span class="o">-&gt;</span><span class="n">Write</span><span class="p">();</span>
</pre></div>
</div>
<p>Generation of the Markov chain from the optimal policy:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">TransitionStructure</span><span class="w"> </span><span class="o">*</span><span class="n">Mat</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mdp</span><span class="o">-&gt;</span><span class="n">GetChain</span><span class="p">(</span><span class="n">optimum</span><span class="p">);</span>
<span class="n">Mat</span><span class="o">-&gt;</span><span class="n">set_type</span><span class="p">(</span><span class="n">DISCRETE</span><span class="p">);</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="o">*</span><span class="n">Mat</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>

<span class="c1">// Creation of the initial distribution</span>
<span class="kt">double</span><span class="w"> </span><span class="n">initial_prob</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">0.333</span><span class="p">,</span><span class="w"> </span><span class="mf">0.333</span><span class="p">,</span><span class="w"> </span><span class="mf">0.334</span><span class="p">};</span>
<span class="kt">int</span><span class="w"> </span><span class="n">states</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">};</span>
<span class="n">DiscreteDistribution</span><span class="w"> </span><span class="o">*</span><span class="n">initial</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">DiscreteDistribution</span><span class="p">(</span><span class="n">states</span><span class="p">,</span><span class="w"> </span><span class="n">initial_prob</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>

<span class="c1">// Creation of the Markov chain</span>
<span class="n">MarkovChain</span><span class="w"> </span><span class="o">*</span><span class="n">chain</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MarkovChain</span><span class="p">(</span><span class="n">Mat</span><span class="p">);</span>
<span class="n">chain</span><span class="o">-&gt;</span><span class="n">set_init_distribution</span><span class="p">(</span><span class="n">initial</span><span class="p">);</span>
<span class="n">chain</span><span class="o">-&gt;</span><span class="n">set_model_name</span><span class="p">(</span><span class="s">&quot;Chain issued from the MDP&quot;</span><span class="p">);</span>

<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="o">*</span><span class="n">chain</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Generation of Q-values for reinforcement learning:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">FeedbackQvalueMDP</span><span class="w"> </span><span class="o">*</span><span class="n">F</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">mdp</span><span class="o">-&gt;</span><span class="n">GetQValue</span><span class="p">(</span><span class="n">optimum</span><span class="p">);</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="o">*</span><span class="n">F</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>

<span class="c1">// For action sampling, reset the random generator</span>
<span class="n">F</span><span class="o">-&gt;</span><span class="n">ResetSeed</span><span class="p">();</span>

<span class="c1">// Sample an action using EpsilonGreedy in state 0</span>
<span class="kt">int</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">F</span><span class="o">-&gt;</span><span class="n">EpsilonGreedyMax</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mf">0.1</span><span class="p">);</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Action with EpsilonGreedy: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>

<span class="c1">// Sample an action using SoftMax in state 2</span>
<span class="n">action</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">F</span><span class="o">-&gt;</span><span class="n">SoftMax</span><span class="p">(</span><span class="mi">2</span><span class="p">);</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Action with SoftMax: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
</section>
<section id="download">
<h2>Download<a class="headerlink" href="#download" title="Link to this heading"></a></h2>
<p>The source file can be downloaded <a class="reference download internal" download="" href="../_downloads/af8bcc978e132b64ee652cfe851217a0/exampleMDP11.cpp"><code class="xref download docutils literal notranslate"><span class="pre">here</span></code></a>.</p>
</section>
<section id="output">
<h2>Output<a class="headerlink" href="#output" title="Link to this heading"></a></h2>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Size :	
3
Begining MDP building
End of building MDP
printing MDP
#############################################
write MDP
MDP type (discrete,continuous): discrete
MDP rule (min,max): max
#############################################
State space size: 3
Action space size: 3
State  dimension: 1
Action dimension: 1
#############################################
Transition matrix per action:
action: 0
0.7  0.3  0  
0  1  0  
0  0  1  
action: 1
1  0  0  
0  0  1  
0  0  1  
action: 2
0.8  0.2  0  
0  1  0  
0.8  0.1  0.1  
#############################################
Reward Matrix (state,action):
7  0  0  
0  -50  -1e+10  
-1e+10  -1e+10  32  
#############################################
#############################################
#############################################
write Infinite Discounted MDP
MDP Criteria : infinite discounted
Discount factor:0.95
#############################################
Printing solution from value iteration
#-- Value Iteration -- Done with 298 iterations and final distance=2.61428e-08
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           21.8992   0
  1           1.17982   1
  2           53.8735   2
#############################################

Checking solutions
#-- Cost policy -- Done with 5 iterations and final distance=2.45996e-08
i= 0 sol= 21.8992
i= 1 sol= 1.17982
i= 2 sol= 53.8735

Modified Policy iteration
#-- Policy Iteration Modified -- Done with 75 iterations and final distance=2.52144e-08
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           21.8992   0
  1           1.17982   1
  2           53.8735   2
#############################################

Gauss Seidel Value Iteration
#-- Value Iteration GS -- Done with 246 iterations and final distance=2.55276e-08
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           21.8992   0
  1           1.17982   1
  2           53.8735   2
#############################################

********************************

Destructing
Destructing 2
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="exampleMDP10.html" class="btn btn-neutral float-left" title="MDP Example 10: Discounted MDP" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="exampleMDP21.html" class="btn btn-neutral float-right" title="Example MDP 21 : Average MDP and Policy handling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, 2024, ajm and eh.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>