Enumerate State Space:

Enumerate action Set:

Enumerate action Set by index:
 Action 0 index of Action : 0
 Action 1 index of Action : 1
 Action 2 index of Action : 2
 Action 3 index of Action : 3

Size :	3
Begining building of MDP
End of building MDP
Writing MDP
#############################################
write MDP
MDP type (discrete,continuous): discrete
MDP rule (min,max): max
#############################################
State space size: 3
Action space size: 4
State  dimension: 1
Action dimension: 1
#############################################
Transition matrix per action:
action: 0
0  0  1  
0  0  0  
0  0  0  
action: 1
0  1  0  
0  0  0  
0  0  0  
action: 2
0  0  0  
1  0  0  
0  0  0  
action: 3
0  0  0  
0  0  0  
0.333333  0.333333  0.333334  
#############################################
Reward Matrix (state,action):
2  1  -1000  -1000  
-1000  -1000  2  -1000  
-1000  -1000  -1000  3  
#############################################
#############################################
#############################################
write Average Discounted MDP
MDP Criteria : average
#############################################

Writing the solution of value iteration after 5 iterates
#-- Value Iteration -- Done with 5 iterations and final distance=0.148148
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :          2.51852
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           12.3704   0
  1           11.7778   2
  2           12.8272   3
#############################################

Writing the solution of value iteration after 12 iterates
#-- Value Iteration -- Done with 12 iterations and final distance=0.00445391
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :          2.50019
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           29.8327   0
  1           29.3348   2
  2           30.3333   3
#############################################

Writing the solution of value iteration
#-- Value Iteration -- Done with 27 iterations and final distance=8.57682e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           67.3333   0
  1           66.8333   2
  2           67.8333   3
#############################################


Checking solutions
# Power method Done. With 1 iterations and  final distance: 4.7148e-07
i= 0 sol= 69.8333
i= 1 sol= 69.3333
i= 2 sol= 70.3333


Writing solution of relative value iteration  with special state 0
#-- Relative Value Iteration -- Done with 27 iterations and final distance=8.57682e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0             -1000   0
  1             -1000   2
  2                 3   3
#############################################

Writing solution relative value iteration with special state 1
#-- Relative Value Iteration -- Done with 27 iterations and final distance=8.57682e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0             -1000   0
  1             -1000   2
  2               3.5   3
#############################################

Writing solution relative value iteration with special state 2
#-- Relative Value Iteration -- Done with 27 iterations and final distance=8.57682e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0             -1000   0
  1             -1000   2
  2               2.5   3
#############################################


Checking solutions
# Power method Done. With 40 iterations and  final distance: 8.01171e-07
i= 0  sol= -398.916
i= 1  sol= -399.416
i= 2  sol= -398.416

Writing solution modified policy iteration
#-- Policy Iteration Modified  -- Done with 10 iterations and final distance=4.7148e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           69.8333   0
  1           69.3333   2
  2           70.3333   3
#############################################


Checking solutions
# Power method Done. With 1 iterations and  final distance: 4.43054e-07
i= 0  sol= 72.3333
i= 1  sol= 71.8333
i= 2  sol= 72.8333


********************************
Destruction
Destruction 2
Destruction 3
Size :	3
Begining building of MDP
End of building MDP
Writing MDP
#############################################
write MDP
MDP type (discrete,continuous): discrete
MDP rule (min,max): max
#############################################
State space size: 3
Action space size: 4
State  dimension: 1
Action dimension: 1
#############################################
Transition matrix per action:
action: 0
0  0  1  
0  0  0  
0  0  0  
action: 1
0  1  0  
0  0  0  
0  0  0  
action: 2
0  0  0  
1  0  0  
0  0  0  
action: 3
0  0  0  
0  0  0  
0.333333  0.333333  0.333334  
#############################################
Reward Matrix (state,action):
2  1  -1000  -1000  
-1000  -1000  2  -1000  
-1000  -1000  -1000  3  
#############################################
#############################################
#############################################
write Average Discounted MDP
MDP Criteria : average
#############################################

Writing the solution of value iteration after 5 iterates
#-- Value Iteration -- Done with 5 iterations and final distance=0.148148
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :          2.51852
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           12.3704   0
  1           11.7778   2
  2           12.8272   3
#############################################

Writing the solution of value iteration after 12 iterates
#-- Value Iteration -- Done with 12 iterations and final distance=0.00445391
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :          2.50019
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           29.8327   0
  1           29.3348   2
  2           30.3333   3
#############################################

Writing the solution of value iteration
#-- Value Iteration -- Done with 27 iterations and final distance=8.57682e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           67.3333   0
  1           66.8333   2
  2           67.8333   3
#############################################


Checking solutions
# Power method Done. With 1 iterations and  final distance: 4.7148e-07
i= 0 sol= 69.8333
i= 1 sol= 69.3333
i= 2 sol= 70.3333


Writing solution of relative value iteration  with special state 0
#-- Relative Value Iteration -- Done with 27 iterations and final distance=8.57682e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0             -1000   0
  1             -1000   2
  2                 3   3
#############################################

Writing solution relative value iteration with special state 1
#-- Relative Value Iteration -- Done with 27 iterations and final distance=8.57682e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0             -1000   0
  1             -1000   2
  2               3.5   3
#############################################

Writing solution relative value iteration with special state 2
#-- Relative Value Iteration -- Done with 27 iterations and final distance=8.57682e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0             -1000   0
  1             -1000   2
  2               2.5   3
#############################################


Checking solutions
# Power method Done. With 40 iterations and  final distance: 8.01171e-07
i= 0  sol= -398.916
i= 1  sol= -399.416
i= 2  sol= -398.416

Writing solution modified policy iteration
#-- Policy Iteration Modified  -- Done with 10 iterations and final distance=4.7148e-07
#Print solution of an MDP problem 
#Size of the state space : 3

#############################################
# Average Cost (only for average models) :              2.5
# Solution of the entered problem model:
# - column 1: index of the state
# - column 2: Value function 
# - column 3: Optimal action 
#
  0           69.8333   0
  1           69.3333   2
  2           70.3333   3
#############################################


Checking solutions
# Power method Done. With 1 iterations and  final distance: 4.43054e-07
i= 0  sol= 72.3333
i= 1  sol= 71.8333
i= 2  sol= 72.8333


********************************
Destruction
Destruction 2
Destruction 3
